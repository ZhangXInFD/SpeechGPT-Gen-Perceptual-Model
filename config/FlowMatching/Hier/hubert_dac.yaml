model_args:
  backbone_kwargs:
    dim: 1024
    depth: 12
    heads: 16
    adaptive_rmsnorm: false
    adaptive_rmsnorm_cond_dim_in: null
    ff_dropout: 0.1
    conv_dropout: 0.1
    attn_dropout: 0.1
    attn_flash: false
  num_semantic_tokens: 1024
  explict: false
  start_from_cond: false
  concat_cond: true
  sigma: 0.0
  ode_atol: 1.0e-7
  ode_rtol: 1.0e-9
  ode_step_size: 0.0625
  use_torchode: false
  torchdiffeq_ode_method: midpoint
  backbone_type: uconformer



trainer_args:
  results_folder: ./Log/hier/hubert_dac
  speechtokenizer_ckpt: /remote-home/xzhang/.cache/descript/dac/weights_16khz_8kbps_0.0.5.pth
  train_file_list: /remote-home/share/data/SpeechPretrain/mls_english/train/mls_hubert_unit_text_pair.txt
  valid_file_list: /remote-home/share/data/SpeechPretrain/mls_english/dev/dev_mls_hubert_unit_text_pair.txt
  # train_file_list: /remote-home/xzhang/audiolm/mhubert/mhubert_unit/train/1.txt
  # valid_file_list: /remote-home/xzhang/audiolm/mhubert/mhubert_unit/dev/1.txt
  train_audio_root: /remote-home/share/data/SpeechPretrain/mls_english/train/audio
  valid_audio_root: /remote-home/share/data/SpeechPretrain/mls_english/dev/audio
  num_warmup_steps: 5000
  batch_size: 8
  epochs: 4
  is_raw_wav: true
  hierarchical: true
  max_sequence: 600
  learning_rate: 1.0e-4
  initial_learning_rate: 1.0e-5
  weight_decay: 0.0
  max_grad_norm: 0.5
  log_steps: 50
  save_model_steps: 1250
  num_ckpt_keep: 8
  split_batches: false
  drop_last: false
  num_workers: 6
  accelerate_kwargs:
    log_with: tensorboard
    gradient_accumulation_steps: 2