model_args:
  conformer_kwargs:
    dim: 1024
    depth: 12
    heads: 16
    adaptive_rmsnorm: false
    adaptive_rmsnorm_cond_dim_in: null
    ff_dropout: 0.1
    conv_dropout: 0.1
    attn_dropout: 0.1
    attn_flash: false
  explict: false
  start_from_cond: true
  concat_cond: false
  sigma: 0.0
  ode_atol: 1.0e-7
  ode_rtol: 1.0e-9
  ode_step_size: 0.0625
  use_torchode: false
  torchdiffeq_ode_method: midpoint
  model_type: uconformer



trainer_args:
  results_folder: ./Log/uconformer/spt_snake
  speechtokenizer_cfg: /remote-home/xzhang/Speech/UniTokenizer/HubertCodec/Log/FullDataset/RVQ_Snake/config.json
  speechtokenizer_ckpt: /remote-home/xzhang/Speech/UniTokenizer/HubertCodec/Log/FullDataset/RVQ_Snake/ckpt/ckpt.pt
  train_file_list: /remote-home/xzhang/Speech/USLM2/mls_train_file_list_wav.txt
  valid_file_list: /remote-home/xzhang/Speech/USLM2/mls_valid_file_list_wav.txt
  num_warmup_steps: 5000
  batch_size: 46
  epochs: 4
  is_raw_wav: true
  max_sequence: 600
  learning_rate: 1.0e-4
  initial_learning_rate: 1.0e-5
  weight_decay: 0.0
  max_grad_norm: 0.5
  log_steps: 50
  save_model_steps: 1250
  num_ckpt_keep: 8
  split_batches: false
  drop_last: false
  num_workers: 8
  accelerate_kwargs:
    log_with: tensorboard
    gradient_accumulation_steps: 2