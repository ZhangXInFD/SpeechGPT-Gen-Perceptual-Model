model_args:
  pad_id: 1024
  num_semantic_token_ids: 1024
  semantic_pad_id: -1
  schedule: linear
  BackboneWrapper:
    codebook_size: 1024
    num_quantizers: 7
    grouped_quantizers: 1
    backbone_type: conformer
    backbone_kwargs:
      dim: 1024
      depth: 12
      heads: 16
      adaptive_rmsnorm: false
      adaptive_rmsnorm_cond_dim_in: null
      ff_dropout: 0.1
      conv_dropout: 0.1
      attn_dropout: 0.1
      attn_flash: false



trainer_args:
  results_folder: /remote-home/share/personal/xzhang/SpeechGPT-gen/soundstorm/speechtokenizer/spt_snake
  speechtokenizer_cfg: /remote-home/xzhang/Speech/UniTokenizer/HubertCodec/Log/FullDataset/RVQ_Snake/config.json
  speechtokenizer_ckpt: /remote-home/xzhang/Speech/UniTokenizer/HubertCodec/Log/FullDataset/RVQ_Snake/ckpt/ckpt.pt
  train_file_list: /remote-home/xzhang/Speech/USLM2/mls_train_file_list_wav.txt
  valid_file_list: /remote-home/xzhang/Speech/USLM2/mls_valid_file_list_wav.txt
  num_warmup_steps: 5000
  batch_size: 8
  epochs: 4
  is_raw_wav: true
  max_sequence: 600
  learning_rate: 1.0e-4
  initial_learning_rate: 1.0e-5
  weight_decay: 0.0
  max_grad_norm: 0.5
  log_steps: 50
  save_model_steps: 1250
  num_ckpt_keep: 8
  split_batches: false
  drop_last: false
  num_workers: 8
  accelerate_kwargs:
    log_with: tensorboard
    gradient_accumulation_steps: 1