model_args:
  pad_id: 1024
  num_semantic_token_ids: 500
  semantic_pad_id: -1
  schedule: linear
  BackboneWrapper:
    codebook_size: 1024
    num_quantizers: 12
    grouped_quantizers: 1
    backbone_type: conformer
    backbone_kwargs:
      dim: 1024
      depth: 12
      heads: 16
      adaptive_rmsnorm: false
      adaptive_rmsnorm_cond_dim_in: null
      ff_dropout: 0.1
      conv_dropout: 0.1
      attn_dropout: 0.1
      attn_flash: false



trainer_args:
  results_folder: /remote-home/share/personal/xzhang/SpeechGPT-gen/soundstorm/hier/hubert_dac
  speechtokenizer_ckpt: /remote-home/xzhang/.cache/descript/dac/weights_16khz_8kbps_0.0.5.pth
  train_file_list: /remote-home/share/data/SpeechPretrain/mls_english/train/mls_hubert_unit_text_pair.txt
  valid_file_list: /remote-home/share/data/SpeechPretrain/mls_english/dev/dev_mls_hubert_unit_text_pair.txt
  # train_file_list: /remote-home/xzhang/audiolm/mhubert/mhubert_unit/train/1.txt
  # valid_file_list: /remote-home/xzhang/audiolm/mhubert/mhubert_unit/dev/1.txt
  train_audio_root: /remote-home/share/data/SpeechPretrain/mls_english/train/audio
  valid_audio_root: /remote-home/share/data/SpeechPretrain/mls_english/dev/audio
  num_warmup_steps: 0
  batch_size: 40
  epochs: 4
  is_raw_wav: true
  hierarchical: true
  max_sequence: 600
  learning_rate: 5.0e-4
  initial_learning_rate: 5.0e-4
  weight_decay: 0.0
  max_grad_norm: 0.5
  log_steps: 50
  save_model_steps: 1250
  num_ckpt_keep: 8
  split_batches: false
  drop_last: false
  num_workers: 6
  accelerate_kwargs:
    log_with: tensorboard
    gradient_accumulation_steps: 1